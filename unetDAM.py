# -*- coding: utf-8 -*-
"""notebookc6488a08cb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jMPW1X4ivnAcXjbAueQs1jsM2WUXvkC7
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
# data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
import tensorflow as tf



print("Num GPUs Available: ", (tf.config.experimental.list_physical_devices('GPU')))

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
import matplotlib.pyplot as plt

#DAM Module

def se_block(residual, name, ratio=8):
    """Contains the implementation of Squeeze-and-Excitation(SE) block.
    As described in https://arxiv.org/abs/1709.01507.
    """

    kernel_initializer = tf.keras.initializers.VarianceScaling()
    bias_initializer = tf.constant_initializer(value=0.0)

    with tf.compat.v1.variable_scope(name):
        channel = residual.shape[-1]
        
        # Global average pooling
        squeeze = tf.reduce_mean(residual, axis=[1, 2], keepdims=True)
        print(squeeze.shape)
        assert squeeze.shape[1:] == (1, 1, channel)
        
        # Bottleneck fully connected layer
        bottleneck_fc = tf.keras.layers.Dense(units=channel//ratio,
                                             activation=tf.nn.relu,
                                             kernel_initializer=kernel_initializer,
                                             bias_initializer=bias_initializer,
                                             name='bottleneck_fc')(squeeze)
        
        print(bottleneck_fc.shape[1:], (1, 1, channel//ratio))
        assert bottleneck_fc.shape[1:] == (1, 1, channel//ratio)
        
        # Recover fully connected layer
        recover_fc = tf.keras.layers.Dense(units=channel,
                                           activation=tf.nn.sigmoid,
                                           kernel_initializer=kernel_initializer,
                                           bias_initializer=bias_initializer,
                                           name='recover_fc')(bottleneck_fc)
        
        assert recover_fc.shape[1:] == (1, 1, channel)
        
        # Scale the residual with the excitation
        scale = residual * recover_fc

    return scale

def cbam_block(input_feature, name, ratio=8):
    with tf.compat.v1.variable_scope(name):
        attention_feature = channel_attention(input_feature, 'ch_at', ratio)
        attention_feature = spatial_attention(attention_feature, 'sp_at')
    print ("CBAM Hello")
    return attention_feature

def channel_attention(input_feature, name, ratio=8):
    kernel_initializer = tf.keras.initializers.VarianceScaling()
    bias_initializer = tf.constant_initializer(value=0.0)

    with tf.compat.v1.variable_scope(name):
        channel = input_feature.shape[-1]

        # Average pooling
        avg_pool = tf.reduce_mean(input_feature, axis=[1, 2], keepdims=True)
        assert avg_pool.shape[1:] == (1, 1, channel)
        
        # Fully connected layer 0
        avg_pool = tf.keras.layers.Dense(units=channel // ratio,
                                         activation=tf.nn.relu,
                                         kernel_initializer=kernel_initializer,
                                         bias_initializer=bias_initializer,
                                         name='mlp_0')(avg_pool)
        
        assert avg_pool.shape[1:] == (1, 1, channel // ratio)
        
        # Fully connected layer 1
        avg_pool = tf.keras.layers.Dense(units=channel,
                                         kernel_initializer=kernel_initializer,
                                         bias_initializer=bias_initializer,
                                         name='mlp_1')(avg_pool)
        
        assert avg_pool.shape[1:] == (1, 1, channel)

        # Max pooling
        max_pool = tf.reduce_max(input_feature, axis=[1,2], keepdims=True)
        assert max_pool.shape[1:] == (1, 1, channel)
        
        # Fully connected layer 0
        max_pool = tf.keras.layers.Dense(units=channel // ratio,
                                         activation=tf.nn.relu,
                                         name='mlp_2')(max_pool)
        
        assert max_pool.shape[1:] == (1, 1, channel // ratio)
        
        # Fully connected layer 1
        max_pool = tf.keras.layers.Dense(units=channel,
                                         name='mlp_3')(max_pool)
        
        assert max_pool.shape[1:] == (1, 1, channel)

        # Sigmoid activation and scale
        scale = tf.sigmoid(avg_pool + max_pool, name='sigmoid')

    return input_feature * scale


def spatial_attention(input_feature, name):
    kernel_size = 7
    kernel_initializer = tf.keras.initializers.VarianceScaling()

    with tf.compat.v1.variable_scope(name):
        avg_pool = tf.reduce_mean(input_feature, axis=[3], keepdims=True)
        print(avg_pool.shape)
        assert avg_pool.shape[-1] == 1
        max_pool = tf.reduce_max(input_feature, axis=[3], keepdims=True)
        print(max_pool.shape)
        assert max_pool.shape[-1] == 1
        concat = tf.concat([avg_pool, max_pool], axis=3)
        print(concat.shape)
        assert concat.shape[-1] == 2
        
        print(concat.shape)
        concat = tf.keras.layers.Conv2D(filters=1,
                                        kernel_size=(kernel_size, kernel_size),
                                        strides=(1, 1),
                                        padding="same",
                                        activation=None,
                                        kernel_initializer=kernel_initializer,
                                        use_bias=False,
                                        name='conv')(tf.cast(concat, dtype=tf.float32))
        #concat = tf.squeeze(concat, axis=0)  # Remove the batch dimension

        assert concat.shape[-1] == 1

        concat = tf.sigmoid(concat, name='sigmoid')

    return input_feature * concat

    













def load_and_preprocess_data(image_path):
    # Read and decode image using OpenCV
    image = cv2.imread(image_path, cv2.IMREAD_COLOR)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format
     # Resize the image to your desired dimensions
    image=cv2.resize(image,(128,128))
    # Normalize image to [0, 1] if needed
    image = image / 255.0

    return image




import numpy as np

def load_and_preprocess_mask(image_path):
    # Convert BGR image to RGB
    image = cv2.imread(image_path, cv2.IMREAD_COLOR)
    # image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image =cv2.resize(image ,(128,128))
    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    one_hot_mask = tf.keras.utils.to_categorical( image_gray/255.0, num_classes=2)


    return one_hot_mask  




# Define data paths
train_data_dir = '/home/arnav/Downloads/CV Project/DeepCrack/train_img'  # Replace with the path to your training data directory
train_mask_dir = '/home/arnav/Downloads/CV Project/DeepCrack/train_lab'  # Replace with the path to your training masks directory



train_data_paths=os.listdir(train_data_dir)
train_mask_paths=os.listdir(train_mask_dir)
print(len(train_data_paths))
train_data_paths.sort()
train_mask_paths.sort()


# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# TrainImg = np.array(TrainImg)
# TestImg = np.array(TestImg)
# TestMask = np.array(TestMask)
# TrainMask = np.array(TrainMask)

# TrainMask.shape
TrainImg=[load_and_preprocess_data('//home/arnav/Downloads/CV Project/DeepCrack/train_img/'+image_path) for image_path in train_data_paths]
TrainMask =[load_and_preprocess_mask("/home/arnav/Downloads/CV Project/DeepCrack/train_lab/"+image_path) for image_path in train_mask_paths]
from sklearn.model_selection import train_test_split
TrainImg, X_Val , TrainMask, Y_Val = train_test_split(TrainImg, TrainMask , test_size=0.2, shuffle=True)

# Define data paths for validation
val_data_dir = '/home/arnav/Downloads/CV Project/DeepCrack/test_img'  # Replace with the path to your validation data directory
val_mask_dir = '/home/arnav/Downloads/CV Project/DeepCrack/test_lab'  # Replace with the path to your validation masks directory

val_data_paths=os.listdir(val_data_dir)
val_mask_paths=os.listdir(val_mask_dir)



# Ensure the order of validation image and mask file paths matches
val_data_paths.sort()
val_mask_paths.sort()


val_img=[load_and_preprocess_data('/home/arnav/Downloads/CV Project/DeepCrack/test_img/'+image_path) for image_path in val_data_paths]
val_mask=[load_and_preprocess_mask('/home/arnav/Downloads/CV Project/DeepCrack/test_lab/'+image_path) for image_path in val_mask_paths]

TestImg =np.array(val_img)
TestMask =np.array(val_mask)
X_Val=np.array(X_Val)
Y_Val=np.array(Y_Val)


# from sklearn.model_selection import train_test_split
# X_Train, X_Val, Y_Train, Y_Val = train_test_split(X, Y , test_size=0.2, stratify=Y)

TrainImg=np.array(TrainImg)
TrainMask=np.array(TrainMask)

print("Shape of TrainImg:", TrainImg.shape)
print("Shape of TrainMask:", TrainMask.shape)
print("Shape of TestImg:", TestImg.shape)
print("Shape of TestMask:", TestMask.shape)

print("Shape of ValImg:", X_Val.shape)
print("Shape of ValMask:",Y_Val.shape)
print(np.unique(TrainMask[0]),TrainMask[0].shape)

def conv(input_tensor, n_filters, kernel_size = 3):

    x = input_tensor
    for i in range(2):
        x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer = 'he_normal', padding = 'same')(x)
        x = tf.keras.layers.Activation('relu')(x)
    return x

def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):
    f = conv(inputs, n_filters=n_filters)
    p = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(f)
    p = tf.keras.layers.Dropout(0.3)(p)
    return f, p

def encoder(inputs):
    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)
    #f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)
    #f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)
    #f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)
    return p1, f1#, f2, f3, f4)

def bottleneck(inputs):
    input1=cbam_block(inputs,name="cbam1",ratio=8)
    input2=se_block(inputs,name="seblock1",ratio=8)
    inputs=input1+input2
    bottle_neck = conv(inputs, n_filters=1024)

    return bottle_neck

def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):
    u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)
    c = tf.keras.layers.concatenate([u, conv_output])
    c = tf.keras.layers.Dropout(dropout)(c)
    c = conv(c, n_filters, kernel_size=3)
    return c

def decoder(inputs, convs, output_channels):
    f1 = convs#, f2, f3, f4 = convs
    c6 = decoder_block(inputs, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)
    #c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)
    #c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)
    #c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)
    outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='sigmoid')(c6)
    return outputs

OUTPUT_CHANNELS = 2

def unet():
    # specify the input shape
    inputs = tf.keras.layers.Input(shape=(128,128, 3))
    # feed the inputs to the encoder
    encoder_output, convs = encoder(inputs)
    # feed the encoder output to the bottleneck
    bottle_neck = bottleneck(encoder_output)
    # feed the bottleneck and encoder block outputs to the decoder
    # specify the number of classes via the `output_channels` argument
    outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)
    # create the model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)
with tf.device('/GPU:0'):
    model = unet()
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    model_history = model.fit(TrainImg,TrainMask, epochs=1000, batch_size=20, validation_split=False, validation_data=(X_Val,Y_Val),shuffle=True,callbacks=[early_stopping])

acc = model.evaluate(TestImg,TestMask)

model.save("unetDAMModel2.h5")
plt.plot(model_history.history["loss"])
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()
print(acc)